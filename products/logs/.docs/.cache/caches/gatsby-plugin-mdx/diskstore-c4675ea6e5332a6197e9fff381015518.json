{"expireTime":9007200898048142000,"key":"gatsby-plugin-mdx-entire-payload-69fd4d55246bff4dd3b9ba1d8ed63063-","val":{"mdast":{"type":"root","children":[{"type":"heading","depth":1,"children":[{"type":"text","value":"Understanding the basics","position":{"start":{"line":2,"column":3,"offset":3},"end":{"line":2,"column":27,"offset":27},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":27,"offset":27},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Access pattern","position":{"start":{"line":4,"column":4,"offset":32},"end":{"line":4,"column":18,"offset":46},"indent":[]}}],"position":{"start":{"line":4,"column":1,"offset":29},"end":{"line":4,"column":18,"offset":46},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The basic access pattern is ","position":{"start":{"line":6,"column":1,"offset":48},"end":{"line":6,"column":29,"offset":76},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"give me all the logs for zone Z for minute M","position":{"start":{"line":6,"column":30,"offset":77},"end":{"line":6,"column":74,"offset":121},"indent":[]}}],"position":{"start":{"line":6,"column":29,"offset":76},"end":{"line":6,"column":75,"offset":122},"indent":[]}},{"type":"text","value":" where the minute ","position":{"start":{"line":6,"column":75,"offset":122},"end":{"line":6,"column":93,"offset":140},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"M","position":{"start":{"line":6,"column":94,"offset":141},"end":{"line":6,"column":95,"offset":142},"indent":[]}}],"position":{"start":{"line":6,"column":93,"offset":140},"end":{"line":6,"column":96,"offset":143},"indent":[]}},{"type":"text","value":" refers to the time the log entries were written to disk in Cloudflare's log aggregation system.","position":{"start":{"line":6,"column":96,"offset":143},"end":{"line":6,"column":192,"offset":239},"indent":[]}}],"position":{"start":{"line":6,"column":1,"offset":48},"end":{"line":6,"column":192,"offset":239},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"To start, try running your query every minute. If responses are too small, go up to 5 minutes as this will be appropriate for most zones. If the responses are too large, try going down to 15 seconds.","position":{"start":{"line":8,"column":1,"offset":241},"end":{"line":8,"column":200,"offset":440},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":241},"end":{"line":8,"column":200,"offset":440},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"If your zone has so many logs that it takes longer than 1 minute to read 1 minute worth of logs, run 2 workers staggered, each requesting 1 minute worth of logs every 2 minutes.","position":{"start":{"line":10,"column":1,"offset":442},"end":{"line":10,"column":178,"offset":619},"indent":[]}}],"position":{"start":{"line":10,"column":1,"offset":442},"end":{"line":10,"column":178,"offset":619},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Data returned by the API will not change on repeat calls. The order of messages in the response may be different, but the number and content of the messages will always be the same for a given query as long as the response code is ","position":{"start":{"line":12,"column":1,"offset":621},"end":{"line":12,"column":232,"offset":852},"indent":[]}},{"type":"inlineCode","value":"200","position":{"start":{"line":12,"column":232,"offset":852},"end":{"line":12,"column":237,"offset":857},"indent":[]}},{"type":"text","value":" and there is no error reading the response body.","position":{"start":{"line":12,"column":237,"offset":857},"end":{"line":12,"column":286,"offset":906},"indent":[]}}],"position":{"start":{"line":12,"column":1,"offset":621},"end":{"line":12,"column":286,"offset":906},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Because our log processing system ingests data in batches, most zones with less than 1 million requests per minute will have \"empty\" minutes. Queries for such a minute result in responses with status ","position":{"start":{"line":14,"column":1,"offset":908},"end":{"line":14,"column":201,"offset":1108},"indent":[]}},{"type":"inlineCode","value":"200","position":{"start":{"line":14,"column":201,"offset":1108},"end":{"line":14,"column":206,"offset":1113},"indent":[]}},{"type":"text","value":" but no data in the body. This does not mean that there were no requests proxied by Cloudflare for that minute. It just means that our system did not process a batch of logs for that zone in that minute.","position":{"start":{"line":14,"column":206,"offset":1113},"end":{"line":14,"column":409,"offset":1316},"indent":[]}}],"position":{"start":{"line":14,"column":1,"offset":908},"end":{"line":14,"column":409,"offset":1316},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Order of the data returned","position":{"start":{"line":16,"column":4,"offset":1321},"end":{"line":16,"column":30,"offset":1347},"indent":[]}}],"position":{"start":{"line":16,"column":1,"offset":1318},"end":{"line":16,"column":30,"offset":1347},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The ","position":{"start":{"line":18,"column":1,"offset":1349},"end":{"line":18,"column":5,"offset":1353},"indent":[]}},{"type":"inlineCode","value":"logs/received","position":{"start":{"line":18,"column":5,"offset":1353},"end":{"line":18,"column":20,"offset":1368},"indent":[]}},{"type":"text","value":" API endpoint exposes data by time received, which is the time the event was written to disk in the Cloudflare Logs aggregation system.","position":{"start":{"line":18,"column":20,"offset":1368},"end":{"line":18,"column":155,"offset":1503},"indent":[]}}],"position":{"start":{"line":18,"column":1,"offset":1349},"end":{"line":18,"column":155,"offset":1503},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Ordering by log aggregation time instead of log generation time results in lower (faster) log pipeline latency and deterministic log pulls. Functionally, it is similar to tailing a log file or reading from ","position":{"start":{"line":20,"column":1,"offset":1505},"end":{"line":20,"column":207,"offset":1711},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"rsyslog","position":{"start":{"line":20,"column":208,"offset":1712},"end":{"line":20,"column":215,"offset":1719},"indent":[]}}],"position":{"start":{"line":20,"column":207,"offset":1711},"end":{"line":20,"column":216,"offset":1720},"indent":[]}},{"type":"text","value":" (albeit in chunks).","position":{"start":{"line":20,"column":216,"offset":1720},"end":{"line":20,"column":236,"offset":1740},"indent":[]}}],"position":{"start":{"line":20,"column":1,"offset":1505},"end":{"line":20,"column":236,"offset":1740},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"This means that to obtain logs for a given time range, you can issue one call for each consecutive minute (or other time range). Because log lines are batched by time received and made available, there is no late arriving data. A response for a given minute will never change. You do not have to repeatedly poll a given time range to receive logs as they converge on our aggregation system.","position":{"start":{"line":22,"column":1,"offset":1742},"end":{"line":22,"column":391,"offset":2132},"indent":[]}}],"position":{"start":{"line":22,"column":1,"offset":1742},"end":{"line":22,"column":391,"offset":2132},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Format of the data returned","position":{"start":{"line":24,"column":4,"offset":2137},"end":{"line":24,"column":31,"offset":2164},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":2134},"end":{"line":24,"column":31,"offset":2164},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The Logpull API returns data in NDJSON format, whereby each log line is a valid JSON object. Major analysis tools like Google BigQuery and AWS Kinesis require this format.","position":{"start":{"line":26,"column":1,"offset":2166},"end":{"line":26,"column":172,"offset":2337},"indent":[]}}],"position":{"start":{"line":26,"column":1,"offset":2166},"end":{"line":26,"column":172,"offset":2337},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"To turn the resulting log data into a JSON array with one array element per log line, you can use the ","position":{"start":{"line":28,"column":1,"offset":2339},"end":{"line":28,"column":103,"offset":2441},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"jq","position":{"start":{"line":28,"column":104,"offset":2442},"end":{"line":28,"column":106,"offset":2444},"indent":[]}}],"position":{"start":{"line":28,"column":103,"offset":2441},"end":{"line":28,"column":107,"offset":2445},"indent":[]}},{"type":"text","value":" tool.  Essentially, you pipe the API response into ","position":{"start":{"line":28,"column":107,"offset":2445},"end":{"line":28,"column":159,"offset":2497},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"jq","position":{"start":{"line":28,"column":160,"offset":2498},"end":{"line":28,"column":162,"offset":2500},"indent":[]}}],"position":{"start":{"line":28,"column":159,"offset":2497},"end":{"line":28,"column":163,"offset":2501},"indent":[]}},{"type":"text","value":" using the ","position":{"start":{"line":28,"column":163,"offset":2501},"end":{"line":28,"column":174,"offset":2512},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"slurp","position":{"start":{"line":28,"column":175,"offset":2513},"end":{"line":28,"column":180,"offset":2518},"indent":[]}}],"position":{"start":{"line":28,"column":174,"offset":2512},"end":{"line":28,"column":181,"offset":2519},"indent":[]}},{"type":"text","value":" (or simply ","position":{"start":{"line":28,"column":181,"offset":2519},"end":{"line":28,"column":193,"offset":2531},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"s","position":{"start":{"line":28,"column":194,"offset":2532},"end":{"line":28,"column":195,"offset":2533},"indent":[]}}],"position":{"start":{"line":28,"column":193,"offset":2531},"end":{"line":28,"column":196,"offset":2534},"indent":[]}},{"type":"text","value":") flag:","position":{"start":{"line":28,"column":196,"offset":2534},"end":{"line":28,"column":203,"offset":2541},"indent":[]}}],"position":{"start":{"line":28,"column":1,"offset":2339},"end":{"line":28,"column":203,"offset":2541},"indent":[]}},{"type":"paragraph","children":[{"type":"inlineCode","value":"<API request data> | jq -s","position":{"start":{"line":30,"column":1,"offset":2543},"end":{"line":30,"column":29,"offset":2571},"indent":[]}}],"position":{"start":{"line":30,"column":1,"offset":2543},"end":{"line":30,"column":29,"offset":2571},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The following is a sample log with default fields:","position":{"start":{"line":32,"column":1,"offset":2573},"end":{"line":32,"column":51,"offset":2623},"indent":[]}}],"position":{"start":{"line":32,"column":1,"offset":2573},"end":{"line":32,"column":51,"offset":2623},"indent":[]}},{"type":"code","lang":"bash","meta":null,"value":"{\n    \"ClientIP\": \"89.163.242.206\",\n    \"ClientRequestHost\": \"www.theburritobot.com\",\n    \"ClientRequestMethod\": \"GET\",\n    \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\",\n    \"EdgeEndTimestamp\": 1506702504461999900,\n    \"EdgeResponseBytes\": 69045,\n    \"EdgeResponseStatus\": 200,\n    \"EdgeStartTimestamp\": 1506702504433000200,\n    \"RayID\": \"3a6050bcbe121a87\"\n}","position":{"start":{"line":34,"column":1,"offset":2625},"end":{"line":46,"column":4,"offset":3008},"indent":[1,1,1,1,1,1,1,1,1,1,1,1]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Data retention period","position":{"start":{"line":48,"column":4,"offset":3013},"end":{"line":48,"column":25,"offset":3034},"indent":[]}}],"position":{"start":{"line":48,"column":1,"offset":3010},"end":{"line":48,"column":25,"offset":3034},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"You can query for logs starting from 1 minute in the past (relative to the actual time that you make the query) and go back at least 3 days and up to 7 days.","position":{"start":{"line":50,"column":1,"offset":3036},"end":{"line":50,"column":158,"offset":3193},"indent":[]}}],"position":{"start":{"line":50,"column":1,"offset":3036},"end":{"line":50,"column":158,"offset":3193},"indent":[]}},{"type":"export","value":"export const _frontmatter = {\"order\":11,\"pcx-content-type\":\"interim\"}","position":{"start":{"line":52,"column":1,"offset":3195},"end":{"line":52,"column":70,"offset":3264},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":52,"column":70,"offset":3264}}},"scopeImports":["import * as React from 'react'"],"scopeIdentifiers":["React"],"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"order\": 11,\n  \"pcx-content-type\": \"interim\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"understanding-the-basics\"\n  }, \"Understanding the basics\"), mdx(\"h2\", {\n    \"id\": \"access-pattern\"\n  }, \"Access pattern\"), mdx(\"p\", null, \"The basic access pattern is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"give me all the logs for zone Z for minute M\"), \" where the minute \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"M\"), \" refers to the time the log entries were written to disk in Cloudflare's log aggregation system.\"), mdx(\"p\", null, \"To start, try running your query every minute. If responses are too small, go up to 5 minutes as this will be appropriate for most zones. If the responses are too large, try going down to 15 seconds.\"), mdx(\"p\", null, \"If your zone has so many logs that it takes longer than 1 minute to read 1 minute worth of logs, run 2 workers staggered, each requesting 1 minute worth of logs every 2 minutes.\"), mdx(\"p\", null, \"Data returned by the API will not change on repeat calls. The order of messages in the response may be different, but the number and content of the messages will always be the same for a given query as long as the response code is \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"200\"), \" and there is no error reading the response body.\"), mdx(\"p\", null, \"Because our log processing system ingests data in batches, most zones with less than 1 million requests per minute will have \\\"empty\\\" minutes. Queries for such a minute result in responses with status \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"200\"), \" but no data in the body. This does not mean that there were no requests proxied by Cloudflare for that minute. It just means that our system did not process a batch of logs for that zone in that minute.\"), mdx(\"h2\", {\n    \"id\": \"order-of-the-data-returned\"\n  }, \"Order of the data returned\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"logs/received\"), \" API endpoint exposes data by time received, which is the time the event was written to disk in the Cloudflare Logs aggregation system.\"), mdx(\"p\", null, \"Ordering by log aggregation time instead of log generation time results in lower (faster) log pipeline latency and deterministic log pulls. Functionally, it is similar to tailing a log file or reading from \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"rsyslog\"), \" (albeit in chunks).\"), mdx(\"p\", null, \"This means that to obtain logs for a given time range, you can issue one call for each consecutive minute (or other time range). Because log lines are batched by time received and made available, there is no late arriving data. A response for a given minute will never change. You do not have to repeatedly poll a given time range to receive logs as they converge on our aggregation system.\"), mdx(\"h2\", {\n    \"id\": \"format-of-the-data-returned\"\n  }, \"Format of the data returned\"), mdx(\"p\", null, \"The Logpull API returns data in NDJSON format, whereby each log line is a valid JSON object. Major analysis tools like Google BigQuery and AWS Kinesis require this format.\"), mdx(\"p\", null, \"To turn the resulting log data into a JSON array with one array element per log line, you can use the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"jq\"), \" tool.  Essentially, you pipe the API response into \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"jq\"), \" using the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"slurp\"), \" (or simply \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"s\"), \") flag:\"), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"<API request data> | jq -s\")), mdx(\"p\", null, \"The following is a sample log with default fields:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"{\\n    \\\"ClientIP\\\": \\\"89.163.242.206\\\",\\n    \\\"ClientRequestHost\\\": \\\"www.theburritobot.com\\\",\\n    \\\"ClientRequestMethod\\\": \\\"GET\\\",\\n    \\\"ClientRequestURI\\\": \\\"/static/img/testimonial-hipster.png\\\",\\n    \\\"EdgeEndTimestamp\\\": 1506702504461999900,\\n    \\\"EdgeResponseBytes\\\": 69045,\\n    \\\"EdgeResponseStatus\\\": 200,\\n    \\\"EdgeStartTimestamp\\\": 1506702504433000200,\\n    \\\"RayID\\\": \\\"3a6050bcbe121a87\\\"\\n}\\n\")), mdx(\"h2\", {\n    \"id\": \"data-retention-period\"\n  }, \"Data retention period\"), mdx(\"p\", null, \"You can query for logs starting from 1 minute in the past (relative to the actual time that you make the query) and go back at least 3 days and up to 7 days.\"));\n}\n;\nMDXContent.isMDXComponent = true;","rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nexport const _frontmatter = {\n  \"order\": 11,\n  \"pcx-content-type\": \"interim\"\n};\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\")\n  return <div {...props}/>\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = \"wrapper\"\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n    <h1 {...{\n      \"id\": \"understanding-the-basics\"\n    }}>{`Understanding the basics`}</h1>\n    <h2 {...{\n      \"id\": \"access-pattern\"\n    }}>{`Access pattern`}</h2>\n    <p>{`The basic access pattern is `}<em parentName=\"p\">{`give me all the logs for zone Z for minute M`}</em>{` where the minute `}<em parentName=\"p\">{`M`}</em>{` refers to the time the log entries were written to disk in Cloudflare's log aggregation system.`}</p>\n    <p>{`To start, try running your query every minute. If responses are too small, go up to 5 minutes as this will be appropriate for most zones. If the responses are too large, try going down to 15 seconds.`}</p>\n    <p>{`If your zone has so many logs that it takes longer than 1 minute to read 1 minute worth of logs, run 2 workers staggered, each requesting 1 minute worth of logs every 2 minutes.`}</p>\n    <p>{`Data returned by the API will not change on repeat calls. The order of messages in the response may be different, but the number and content of the messages will always be the same for a given query as long as the response code is `}<inlineCode parentName=\"p\">{`200`}</inlineCode>{` and there is no error reading the response body.`}</p>\n    <p>{`Because our log processing system ingests data in batches, most zones with less than 1 million requests per minute will have \"empty\" minutes. Queries for such a minute result in responses with status `}<inlineCode parentName=\"p\">{`200`}</inlineCode>{` but no data in the body. This does not mean that there were no requests proxied by Cloudflare for that minute. It just means that our system did not process a batch of logs for that zone in that minute.`}</p>\n    <h2 {...{\n      \"id\": \"order-of-the-data-returned\"\n    }}>{`Order of the data returned`}</h2>\n    <p>{`The `}<inlineCode parentName=\"p\">{`logs/received`}</inlineCode>{` API endpoint exposes data by time received, which is the time the event was written to disk in the Cloudflare Logs aggregation system.`}</p>\n    <p>{`Ordering by log aggregation time instead of log generation time results in lower (faster) log pipeline latency and deterministic log pulls. Functionally, it is similar to tailing a log file or reading from `}<em parentName=\"p\">{`rsyslog`}</em>{` (albeit in chunks).`}</p>\n    <p>{`This means that to obtain logs for a given time range, you can issue one call for each consecutive minute (or other time range). Because log lines are batched by time received and made available, there is no late arriving data. A response for a given minute will never change. You do not have to repeatedly poll a given time range to receive logs as they converge on our aggregation system.`}</p>\n    <h2 {...{\n      \"id\": \"format-of-the-data-returned\"\n    }}>{`Format of the data returned`}</h2>\n    <p>{`The Logpull API returns data in NDJSON format, whereby each log line is a valid JSON object. Major analysis tools like Google BigQuery and AWS Kinesis require this format.`}</p>\n    <p>{`To turn the resulting log data into a JSON array with one array element per log line, you can use the `}<em parentName=\"p\">{`jq`}</em>{` tool.  Essentially, you pipe the API response into `}<em parentName=\"p\">{`jq`}</em>{` using the `}<em parentName=\"p\">{`slurp`}</em>{` (or simply `}<em parentName=\"p\">{`s`}</em>{`) flag:`}</p>\n    <p><inlineCode parentName=\"p\">{`<API request data> | jq -s`}</inlineCode></p>\n    <p>{`The following is a sample log with default fields:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-bash\"\n      }}>{`{\n    \"ClientIP\": \"89.163.242.206\",\n    \"ClientRequestHost\": \"www.theburritobot.com\",\n    \"ClientRequestMethod\": \"GET\",\n    \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\",\n    \"EdgeEndTimestamp\": 1506702504461999900,\n    \"EdgeResponseBytes\": 69045,\n    \"EdgeResponseStatus\": 200,\n    \"EdgeStartTimestamp\": 1506702504433000200,\n    \"RayID\": \"3a6050bcbe121a87\"\n}\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"data-retention-period\"\n    }}>{`Data retention period`}</h2>\n    <p>{`You can query for logs starting from 1 minute in the past (relative to the actual time that you make the query) and go back at least 3 days and up to 7 days.`}</p>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}